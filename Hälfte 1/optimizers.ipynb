{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# collab\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/stonehenge0/AI-verstehen-Winteraka-2024-25/blob/main/H%C3%A4lfte%201/optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "\n",
    "# Optimization\n",
    "\n",
    "In today's material focuses on understanding the standard methods of optimisation in machine learning. You're going to learn about the training loop and different optimizers.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "In the preparation work, you have seen how backpropagation works. Today, we're going to use the gradients produced by backpropagation for optimizing a loss function using gradient descent.\n",
    "\n",
    "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
    "\n",
    "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
    "\n",
    "$$ \\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1}) $$\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n",
    "\n",
    "The learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n",
    "\n",
    "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
    "\n",
    "In addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n",
    "\n",
    "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1801.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n",
    "\n",
    "In the second part of today's material, you will learn about various strategies for searching over hyperparameters.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
    "\n",
    "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
    "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
    "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
    "\n",
    "The class `torch.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n",
    "\n",
    "## Batch Size\n",
    "\n",
    "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
    "\n",
    "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
    "\n",
    "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
    "\n",
    "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
    "\n",
    "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
    "\n",
    "## Computing Gradients in PyTorch\n",
    "\n",
    "Recall that gradients are only saved for `Tensor`s for which `requires_grad=True`. For convenience, `nn.Parameter` automatically sets `requires_grad=True` on the wrapped `Tensor`. As you call `torch` functions, PyTorch tracks the relevant information needed in case you call `backward` later on, at which point it does the actual computation to compute the gradient and stores it in the `Tensor`'s `grad` field.\n",
    "\n",
    "Also recall that PyTorch accumulates gradients across multiple `backward` calls. So if your tensor's `grad` already contains a value, after calling `backward` again it will have the sum of the original value and the new gradient. This behavior comes in handy in many situations, such as computing gradients over multiple runs on a GPU as part of a single batch. Suppose you choose a batch size of 32, but only 8 inputs fit on your GPU. A typical loss function for a batch computes the sum of losses over each example, so you can compute the losses 8 at a time and sum their gradients, producing the same result as running all 32 inputs at once.\n",
    "\n",
    "### Stopping gradients with `torch.no_grad` or `torch.inference_mode`\n",
    "\n",
    "You may not want PyTorch to track gradients for some computations despite involving tensors with `requires_grad=True`. In this case, you can wrap the computation in the `with torch.inference_mode()` context to prevent this tracking.\n",
    "\n",
    "\n",
    "In this notebook, we will discuss the most popular optimizers used in training neural networks. You will implement optimizers from simple to compex:\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- SGD with Momentum\n",
    "- RMSprop\n",
    "- Adam\n",
    "\n",
    "> ## Learning Objectives\n",
    "> - Understand what optimizers are and why they are used\n",
    "> - Get an intuition for momentum and RMS\n",
    "> - Underatand what the differnet hyperparameters in Adam are and how they affect the training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test funcitons\n",
    "\n",
    "Just execute the code block bellow, without looking at it. Those funcitons are just there to check your own implementations against the behaciour of the torch original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.figure\n",
    "import torch as t\n",
    "from einops import repeat\n",
    "import numpy as np\n",
    "\n",
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "def optimize_function(\n",
    "    function: callable, parameters: t.Tensor, optimizer, n_steps: int\n",
    ") -> List[t.Tensor]:\n",
    "    trajectory = []\n",
    "    for _ in range(n_steps):\n",
    "        trajectory.append(parameters.detach().clone())\n",
    "        loss = function(*parameters)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    trajectory = t.stack(trajectory).float()\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def test_SGD(optimizer_class):\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    N_steps = 100\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optimizer_class(parameters, learning_rate)\n",
    "\n",
    "    test_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    solution_optimizer = t.optim.SGD([parameters], lr=learning_rate)\n",
    "    solution_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, solution_optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    assert t.allclose(test_trajectory, solution_trajectory, atol=1e-3)\n",
    "\n",
    "    print(\"SGD test passed\")\n",
    "\n",
    "\n",
    "def test_momentum(optimizer_class):\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    N_steps = 100\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optimizer_class(parameters, learning_rate, momentum=0.9)\n",
    "\n",
    "    test_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    adjested_learning_rate = learning_rate * (1 - 0.9)\n",
    "    solution_optimizer = t.optim.SGD(\n",
    "        [parameters], lr=adjested_learning_rate, momentum=0.9\n",
    "    )\n",
    "    solution_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, solution_optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    assert t.allclose(test_trajectory, solution_trajectory, atol=1e-3)\n",
    "\n",
    "    print(\"Momentum test passed\")\n",
    "\n",
    "\n",
    "def test_RMSprop(optimizer_class):\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    N_steps = 100\n",
    "    learning_rate = 0.001\n",
    "    epsilon = 1e-8\n",
    "    optimizer = optimizer_class(\n",
    "        parameters, learning_rate, beta=0.9, epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    test_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    solution_optimizer = t.optim.RMSprop(\n",
    "        [parameters], lr=learning_rate, alpha=0.9, eps=epsilon\n",
    "    )\n",
    "    solution_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, solution_optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    assert t.allclose(test_trajectory, solution_trajectory, atol=1e-3)\n",
    "\n",
    "    print(\"RMSprop test passed\")\n",
    "\n",
    "\n",
    "def test_Adam(optimizer_class):\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    N_steps = 100\n",
    "    learning_rate = 0.001\n",
    "    epsilon = 1e-8\n",
    "    optimizer = optimizer_class(\n",
    "        parameters,\n",
    "        learning_rate,\n",
    "        momentum_grad=0.9,\n",
    "        momentum_grad_squared=0.999,\n",
    "        epsilon=epsilon,\n",
    "    )\n",
    "\n",
    "    test_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "    solution_optimizer = t.optim.Adam(\n",
    "        [parameters], lr=learning_rate, betas=(0.9, 0.999)\n",
    "    )\n",
    "    solution_trajectory = optimize_function(\n",
    "        rosenbrocks_banana, parameters, solution_optimizer, N_steps\n",
    "    )\n",
    "\n",
    "    assert t.allclose(test_trajectory, solution_trajectory, atol=1e-3)\n",
    "\n",
    "    print(\"Adam test passed\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_rosenbrock(\n",
    "    trajectories={}, xmin=-2, xmax=2, ymin=-1, ymax=3, n_points=50\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"Plot the rosenbrocks_banana function in 3D and its contour plot over the specified domain with trajectories.\"\"\"\n",
    "\n",
    "    global_minimum = t.tensor([1, 1])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 3D plot\n",
    "    ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "    x = t.linspace(xmin, xmax, n_points)\n",
    "    y = t.linspace(ymin, ymax, n_points)\n",
    "    xx = repeat(x, \"x -> y x\", y=n_points)\n",
    "    yy = repeat(y, \"y -> y x\", x=n_points)\n",
    "    zs = rosenbrocks_banana(xx, yy)\n",
    "    ax1.plot_surface(xx, yy, zs, cmap=\"viridis\", alpha=0.5)\n",
    "\n",
    "    for label, trajectory in trajectories.items():\n",
    "        ax1.plot(\n",
    "            trajectory[:, 0],\n",
    "            trajectory[:, 1],\n",
    "            rosenbrocks_banana(trajectory[:, 0], trajectory[:, 1]),\n",
    "            label=label,\n",
    "            linewidth=4.0,\n",
    "        )\n",
    "    # plot the global minimum\n",
    "    ax1.scatter(\n",
    "        *global_minimum,\n",
    "        rosenbrocks_banana(*global_minimum),\n",
    "        color=\"red\",\n",
    "        label=\"Global Minimum\"\n",
    "    )\n",
    "\n",
    "    ax1.set(xlabel=\"x\", ylabel=\"y\", zlabel=\"z\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Contour plot\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    zs = rosenbrocks_banana(xx, yy)\n",
    "\n",
    "    levels = np.logspace(np.log10(zs.min()), np.log10(zs.max()), 10)\n",
    "    contour = ax2.contour(x, y, zs, levels=levels, cmap=\"viridis\")\n",
    "\n",
    "    cbar = fig.colorbar(contour, ax=ax2)\n",
    "    cbar.ax.set_ylabel(\"Function Value\")\n",
    "\n",
    "    for label, trajectory in trajectories.items():\n",
    "        ax2.plot(trajectory[:, 0], trajectory[:, 1], label=label, linewidth=4.0)\n",
    "\n",
    "    # plot the global minimum\n",
    "    ax2.scatter(*global_minimum, color=\"red\", label=\"Global Minimum\")\n",
    "\n",
    "    ax2.set(xlabel=\"x\", ylabel=\"y\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "The job of an optimizer is to find the minimum in a Loss funciton. The loss function, we are going to consider here is Rosenbrocks Banana function. \n",
    "Of cause, in practice, loss funcitons are verry high dimensional and not so simple. But we have an easier time plotting 2D functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "trajectories = {}\n",
    "plot_rosenbrock(trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement the training loop and Stochastic Gradient Descent (SGD) optimizer\n",
    "(2-5 mins)\n",
    "\n",
    "The training loop consists of the following steps:\n",
    " - Compute the loss\n",
    "- Compute the gradients\n",
    "- Update the parameters\n",
    "- zero the gradients\n",
    "\n",
    "The update rule for Stochastic Gradient Descent (SGD) is given by:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
    "\n",
    "where:\n",
    "- $\\theta_t$ represents the model's parameters at iteration $t$.\n",
    "- $\\alpha$ is the learning rate, which determines the step size of the parameter updates.\n",
    "- $\\nabla_\\theta \\text{Loss}(\\theta_t)$ denotes the gradient of the loss function $\\text{Loss}$ with respect to the parameters $\\theta_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_function( function: callable, parameters: t.Tensor, optimizer, n_steps: int) -> List[t.Tensor]:\n",
    "    trajectory = []\n",
    "    for _ in range(n_steps):\n",
    "        trajectory.append(parameters.detach().clone())\n",
    "        # YOUR CODE HERE\n",
    "       \n",
    "\n",
    "    trajectory = t.stack(trajectory).float()\n",
    "    return trajectory\n",
    "\n",
    "class StocasticGradientDescent:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            # YOUR CODE HERE\n",
    "   \n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "test_SGD(StocasticGradientDescent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution Training loop</summary>\n",
    "        \n",
    "\n",
    "```python\n",
    "def optimize_function( function: callable, parameters: t.Tensor, optimizer, n_steps: int) -> List[t.Tensor]:\n",
    "    trajectory = []\n",
    "    for _ in range(n_steps):\n",
    "        trajectory.append(parameters.detach().clone())\n",
    "        loss = function(*parameters)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    trajectory = t.stack(trajectory).float()\n",
    "    return trajectory\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution SGD</summary>\n",
    "        \n",
    "\n",
    "```python\n",
    "class StocasticGradientDescent:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            self.parameters -= self.learning_rate * self.parameters.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "\n",
    "N_steps = 100\n",
    "learning_rate = 0.001\n",
    "optimizer = StocasticGradientDescent(parameters, learning_rate)\n",
    "\n",
    "trajectories[\"SGD\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
    "plot_rosenbrock(trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement the training loop and Stochastic Gradient Descent with Momentum optimizer\n",
    "(2-5 mins)\n",
    "\n",
    "When we have momentum in our optimizer, we additioally keep track of the \"velocity\" with wich we are currently moving $m_t$. The update rule for Stochastic Gradient Descent with Momentum is given by:\n",
    "\n",
    "$$m_{t+1} = \\beta \\cdot m_t + (1 - \\beta) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot m_{t+1}$$\n",
    "\n",
    "where:\n",
    "- $\\beta$ is the momentum parameter, which determines how much of the previous velocity we keep.\n",
    "- $m_t$ represents the running average of the velocity at iteration $t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, momentum: float):\n",
    "        self.parameters = parameters\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "\n",
    "test_momentum(Momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution Momentum</summary>\n",
    "        \n",
    "\n",
    "```python\n",
    "class Momentum:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, momentum: float):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.average_grad = t.zeros_like(parameters)\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            self.average_grad = self.momentum * self.average_grad + (1 - self.momentum) * self.parameters.grad\n",
    "            self.parameters -= self.learning_rate * self.average_grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "N_steps = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = Momentum(parameters, learning_rate, momentum)\n",
    "trajectories[\"Momentum\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
    "plot_rosenbrock(trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement RMSprop optimizer\n",
    "(2-5 mins)\n",
    "\n",
    "RMSprop is an optimizer that adapts the learning rate for each parameter. To do that, we keep track of the moving average of the squared gradients $r_t$ and update the parameters as follows:\n",
    "\n",
    "$$r_{t+1} = \\beta \\cdot r_t + (1 - \\beta) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{r_{t+1} + \\epsilon}} \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
    "\n",
    "where:\n",
    "- $\\beta$ is the momentum parameter, which determines how much of the previous squared gradients we keep.\n",
    "- $r_t$ represents the moving average of the squared gradients at iteration $t$.\n",
    "- $\\epsilon$ is a small value added to the denominator to avoid division by zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, momentum: float, epsilon: float):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "\n",
    "\n",
    "test_RMSprop(RMSProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution RMSprop</summary>\n",
    "        \n",
    "\n",
    "```python\n",
    "class RMSProp:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, beta: float, epsilon: float):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.squared_gradients = t.zeros_like(parameters)\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            self.squared_gradients = self.beta * self.squared_gradients + (1 - self.beta) * self.parameters.grad ** 2\n",
    "            self.parameters -= self.learning_rate / t.sqrt(self.squared_gradients + self.epsilon) * self.parameters.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "N_steps = 100\n",
    "learning_rate = 0.2\n",
    "beta = 0.9\n",
    "epsilon = 1e-8\n",
    "optimizer = RMSProp(parameters, learning_rate, beta, epsilon)\n",
    "trajectories[\"RMSProp\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
    "\n",
    "plot_rosenbrock(trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement Adam optimizer\n",
    "(2-5 mins)\n",
    "\n",
    "For the Adam optimizer, we now bring both momentum and RMSprop together. We keep track of the moving average of the gradients $m_t$ and the moving average of the squared gradients $r_t$ and update the parameters as follows:\n",
    "\n",
    "$$m_{t+1} = \\beta_1 \\cdot m_t + (1 - \\beta_1) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
    "$$r_{t+1} = \\beta_2 \\cdot r_t + (1 - \\beta_2) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)^2$$\n",
    "$$\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^{t+1}}$$\n",
    "$$\\hat{r}_{t+1} = \\frac{r_{t+1}}{1 - \\beta_2^{t+1}}$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{r}_{t+1} + \\epsilon}} \\cdot \\hat{m}_{t+1}$$\n",
    "\n",
    "where:\n",
    "- $\\beta_1$ and $\\beta_2$ are the momentum parameters for the gradients and squared gradients, respectively.\n",
    "- $m_t$ and $r_t$ represent the moving averages of the gradients and squared gradients at iteration $t$.\n",
    "- $\\hat{m}_{t+1}$ and $\\hat{r}_{t+1}$ are the bias-corrected moving averages.\n",
    "- $\\epsilon$ is a small value added to the denominator to avoid division by zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, momentum_grad: float, momentum_grad_squared: float, epsilon: float):\n",
    "        # YOUR CODE HERE\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            self.t += 1\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "\n",
    "\n",
    "test_Adam(Adam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution Adam</summary>\n",
    "        \n",
    "\n",
    "```python\n",
    "class Adam:\n",
    "    def __init__(self, parameters: t.Tensor, learning_rate: float, momentum_grad: float, momentum_grad_squared: float, epsilon: float):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_grad = momentum_grad\n",
    "        self.momentum_grad_squared = momentum_grad_squared\n",
    "        self.epsilon = epsilon\n",
    "        self.average_grad = t.zeros_like(parameters)\n",
    "        self.average_squared_grad = t.zeros_like(parameters)\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        with t.no_grad():\n",
    "            self.t += 1\n",
    "            self.average_grad = self.momentum_grad * self.average_grad + (1 - self.momentum_grad) * self.parameters.grad\n",
    "            self.average_squared_grad = self.momentum_grad_squared * self.average_squared_grad + (1 - self.momentum_grad_squared) * self.parameters.grad ** 2\n",
    "            average_grad_hat = self.average_grad / (1 - self.momentum_grad ** self.t)\n",
    "            average_squared_grad_hat = self.average_squared_grad / (1 - self.momentum_grad_squared ** self.t)\n",
    "            self.parameters -= self.learning_rate * average_grad_hat / (t.sqrt(average_squared_grad_hat) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameters.grad.zero_()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = t.tensor([-1.0, 2.0], requires_grad=True)\n",
    "\n",
    "N_steps = 100\n",
    "learning_rate = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "optimizer = Adam(parameters, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "trajectories[\"Adam\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
    "plot_rosenbrock(trajectories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS Exercise - Play around with the hyperparameters\n",
    "\n",
    "Play around with learning rates, the betas and epsilon.\n",
    "\n",
    "Challange until time is up: get within a 1e-2 range of the minimum of the banana function in as few steps as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
