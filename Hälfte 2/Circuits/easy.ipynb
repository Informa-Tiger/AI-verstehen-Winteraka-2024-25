{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install circuitsvis\n",
    "    %pip install transformer_lens\n",
    "\n",
    "    # Code to make sure output widgets display\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
    "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/transformer_interp/Circuits/*'\n",
    "    !mv --no-clobber ML4G-2.0-master/workshops/transformer_interp/Circuits/* .\n",
    "    !rm -r ML4G-2.0-master\n",
    "\n",
    "    print(\"Imports & installations complete!\")\n",
    "\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple, Dict, Any, Union\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "\n",
    "\n",
    "def logit_attribution(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    cache: ActivationCache,\n",
    "    token_position: int,\n",
    ") -> Float[Tensor, \"layers heads\"]:\n",
    "    \"\"\"\n",
    "    Computes the logit attribution for a specific token position in the input sequence.\n",
    "\n",
    "    Args:\n",
    "        tokens (Int[Tensor, \"batch seq\"]): The input token IDs tensor with shape (batch_size, sequence_length).\n",
    "        model (HookedTransformer): The HookedTransformer model instance.\n",
    "        cache (ActivationCache): The activation cache containing the intermediate results.\n",
    "        token_position (int): The position of the token in the input sequence for which to compute the attribution.\n",
    "\n",
    "    Returns:\n",
    "        Float[Tensor, \"layers heads\"]: The logit attribution tensor with shape (num_layers, num_heads).\n",
    "\n",
    "    Description:\n",
    "        This function computes the logit attribution for a specific token position in the input sequence.\n",
    "        It unembeds the output of each attention head in each layer, and sees what upweight it gives on the correct next token.\n",
    "\n",
    "    Note:\n",
    "        - The input `tokens` tensor is assumed to have a batch size of 1.\n",
    "        - The `token_position` is zero-indexed, meaning the first token in the sequence has a position of 0.\n",
    "        - The returned attention pattern has shape (num_layers, num_heads), representing the attribution scores\n",
    "          for each layer and attention head.\n",
    "    \"\"\"\n",
    "    # Retrieve the attention results from the activation cache for each transformer block\n",
    "    results = [cache[f\"blocks.{i}.attn.hook_result\"] for i in range(len(model.blocks))]\n",
    "\n",
    "    # Stack the attention results along the layer dimension\n",
    "    results = t.stack(results, dim=1)\n",
    "\n",
    "    # Select the attention results corresponding to the specified token position\n",
    "    results = results[token_position, :, :, :]\n",
    "\n",
    "    # Pass the selected attention results through the model's unembed function to obtain the logits\n",
    "    logits = model.unembed(results)\n",
    "\n",
    "    # Get the ID of the next token in the sequence\n",
    "    next_token_id = tokens[0, token_position + 1]\n",
    "\n",
    "    # Extract the logits corresponding to the next token ID\n",
    "    attention_pattern = logits[:, :, next_token_id]\n",
    "\n",
    "    return attention_pattern\n",
    "\n",
    "\n",
    "def test_logit_attribution(implementation, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    attention_pattern = logit_attribution(tokens, model, cache, token_position=3)\n",
    "    comparison_pattern = implementation(tokens, model, cache, token_position=3)\n",
    "\n",
    "    assert t.allclose(attention_pattern, comparison_pattern)\n",
    "    print(\"Logit attribution test passed!\")\n",
    "\n",
    "\n",
    "def average_over_condition(tensor, condition):\n",
    "    I, J, K = tensor.shape\n",
    "    return [\n",
    "        sum(tensor[i, j, k] for j in range(J) for k in range(K) if condition(j, k))\n",
    "        / sum(condition(j, k) for j in range(J) for k in range(K))\n",
    "        for i in range(I)\n",
    "    ]\n",
    "\n",
    "\n",
    "def over_threshhold_attn(cache, condition, threshhold=0.5, sorce=\"pattern\"):\n",
    "    return_values = []\n",
    "\n",
    "    for layer, pattern in enumerate(cache.stack_activation(\"pattern\")):\n",
    "        scores = average_over_condition(pattern, condition)\n",
    "        indices = [i for i, s in enumerate(scores) if s > threshhold]\n",
    "        for i in indices:\n",
    "            return_values.append(f\"L{layer+1}H{i}\")\n",
    "    return return_values\n",
    "\n",
    "\n",
    "def current_attn_detector(cache: ActivationCache, threshhold=0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return i == j\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache, threshhold=0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return i - j == 1\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache, threshhold=0.3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return j == 0\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def find_repeating_rows(tensor):\n",
    "    \"\"\"\n",
    "    Finds repeating rows (vectors) in a 2D torch tensor.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): A 2D torch tensor.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the indices of repeating rows,\n",
    "          and values are the indices where those rows last occurred.\n",
    "    \"\"\"\n",
    "    last_occurrence = {}\n",
    "    repeats = {}\n",
    "\n",
    "    for pos, token in enumerate(tensor[0]):\n",
    "        id = token.item()\n",
    "\n",
    "        if id in last_occurrence:\n",
    "            repeats[pos] = last_occurrence[id]\n",
    "        last_occurrence[id] = pos\n",
    "\n",
    "    return repeats\n",
    "\n",
    "\n",
    "def induction_attn_detector(\n",
    "    cache: ActivationCache, tokens, off_by_one=True, threshhold=0.3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
    "\n",
    "    Remember - the tokens used to generate rep_cache are (bos_token, *rand_tokens, *rand_tokens)\n",
    "    \"\"\"\n",
    "    repeat_dict = find_repeating_rows(t.tensor(tokens))\n",
    "\n",
    "    def cond(i, j):\n",
    "        if i not in repeat_dict.keys():\n",
    "            return False\n",
    "        to_add = 1 if off_by_one else 0\n",
    "        return repeat_dict[i] + to_add == j\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def test_average_over_condition(implementations):\n",
    "    tensor = t.tensor(\n",
    "        [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [0.1, 0.2, 0.3]]]\n",
    "    )\n",
    "    condition = lambda i, j: i == j\n",
    "    result = average_over_condition(tensor, condition)\n",
    "    comparison = implementations(tensor, condition)\n",
    "    assert t.allclose(result, comparison)\n",
    "    print(\"Average over condition test passed!\")\n",
    "\n",
    "\n",
    "def test_current_attn_detector(implementations, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    result = current_attn_detector(cache)\n",
    "    comparison = implementations(cache)\n",
    "    assert result == comparison\n",
    "    print(\"Current attention detector test passed!\")\n",
    "\n",
    "\n",
    "def test_prev_attn_detector(implementations, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    result = prev_attn_detector(cache)\n",
    "    comparison = implementations(cache)\n",
    "    assert result == comparison\n",
    "    print(\"Previous attention detector test passed!\")\n",
    "\n",
    "\n",
    "def test_first_attn_detector(implementations, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    result = first_attn_detector(cache)\n",
    "    comparison = implementations(cache)\n",
    "    assert result == comparison\n",
    "    print(\"First attention detector test passed!\")\n",
    "\n",
    "\n",
    "def test_induction_attn_detector(implementations, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    result = induction_attn_detector(cache, tokens)\n",
    "    comparison = implementations(cache, tokens)\n",
    "    assert result == comparison\n",
    "    print(\"Induction attention detector test passed!\")\n",
    "\n",
    "\n",
    "def test_induction_attn_detector(implementations, model):\n",
    "    test_string = \"The quick brown fox jumps over the lazy dog.The quick brown fox jumps over the lazy dog.\"\n",
    "    tokens = model.to_tokens(test_string)\n",
    "    _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "    result = induction_attn_detector(cache, tokens)\n",
    "    comparison = implementations(cache, tokens)\n",
    "    assert result == comparison\n",
    "    print(\"Induction attention detector test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/stonehenge0/AI-verstehen-Winteraka-2024-25/blob/main/H%C3%A4lfte%202/Circuits/easy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "## Introduction\n",
    "\n",
    "Thanks to Callum McDougall for creating mauch of this notebooks content!\n",
    "\n",
    "This exercise should get you familiar with concepts in circuits interpretability in transformers.\n",
    "\n",
    "## Content & Learning Objectives\n",
    "\n",
    "#### 1️⃣ Find Inductive Heads via attention patterns\n",
    "\n",
    "You'll plot attnetion patterns to check out theory of Inductive Heads, and see if you can find them in the model you're working with.\n",
    "> ##### Learning objectives\n",
    "> \n",
    "> - Use `circuitsvis` to visualise attention heads\n",
    "> - Understand what the theory of inductive heads predicts about attention patterns\n",
    "> - Use attention patterns to identify inductive heads\n",
    "> - Automate this process to find inductive heads in a larger model\n",
    "\n",
    "#### 2️⃣ Logit Attribution\n",
    "\n",
    "Here, you'll learn how to use TransfomerLens to implement LogitLens, a tool for attributing logit values to specific components of the model. You'll also learn how to use this tool to identify basic attention heads, that are imortant for Induction tasks\n",
    "> ##### Learning objectives\n",
    "> - Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way\n",
    "> - Crosscheck your earlier restuls with the results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import circuitsvis as cv\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tests import test_logit_attribution, test_current_attn_detector, test_average_over_condition, test_first_attn_detector, test_induction_attn_detector, test_prev_attn_detector\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Running Models\n",
    "\n",
    "We will be using two models today:\n",
    "\n",
    "GPT-2 small, a smaller version of the GPT-2 model, with 12 layers and 80 million parameters.\n",
    "\n",
    "A two-layer attention only transfomer model.\n",
    "\n",
    "This toy model is a 2L attention-only transformer trained specifically for today. Some changes to make them easier to interpret:\n",
    "- It has only attention blocks.\n",
    "- The positional embeddings are only added to the residual stream before each key and query vector in the attention layers as opposed to the token embeddings - i.e. we compute queries as `Q = (resid + pos_embed) @ W_Q + b_Q` and same for keys, but values as `V = resid @ W_V + b_V`. This means that **the residual stream can't directly encode positional information**.\n",
    "    - This turns out to make it *way* easier for induction heads to form, it happens 2-3x times earlier - [see the comparison of two training runs](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-11-08-83---VmlldzoyNTI0MDMz?accessToken=8ap8ir6y072uqa4f9uinotdtrwmoa8d8k2je4ec0lyasf1jcm3mtdh37ouijgdbm) here. (The bump in each curve is the formation of induction heads.)\n",
    "    - The argument that does this below is `positional_embedding_type=\"shortformer\"`.\n",
    "- It has no MLP layers, no LayerNorms, and no biases.\n",
    "- There are separate embed and unembed matrices (i.e. the weights are not tied).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "gpt2_small.set_use_attn_result(True)\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True,  # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None,  # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\",\n",
    ")\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "toy_transformer: HookedTransformer = HookedTransformer(cfg)\n",
    "\n",
    "toy_transformer.load_state_dict(t.load(weights_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Caching all Activations\n",
    "\n",
    "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.\n",
    "\n",
    "<details>\n",
    "<summary>Aside - a note on <code>remove_batch_dim</code></summary>\n",
    "\n",
    "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. \n",
    "\n",
    "`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Harry James Potter Evans-Verres went to ML4good, to learn about transformers, that is why Harry James Potter Evans-Verres did this notebook in ML4good.\"\n",
    "\n",
    "gpt_tokens = gpt2_small.to_tokens(text)\n",
    "toymodel_tokens = toy_transformer.to_tokens(text)\n",
    "\n",
    "gpt_logits, gpt_cache = gpt2_small.run_with_cache(gpt_tokens, remove_batch_dim=True)\n",
    "toymodel_logits, toymodel_cache = toy_transformer.run_with_cache(toymodel_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the `gpt_cache` object, you should see that it contains a very large number of keys, each one corresponding to a different activation in the model. You can access the keys by indexing the cache directly, or by a more convenient indexing shorthand. For instance, the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_attn_patterns_layer_0 = gpt_cache[\"pattern\", 0]\n",
    "toymodel_attn_patterns_layer_0 = toymodel_cache[\"pattern\", 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the same thing as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_attn_patterns_layer_0_copy = gpt_cache[\"blocks.0.attn.hook_pattern\"]\n",
    "\n",
    "t.testing.assert_close(gpt_attn_patterns_layer_0, gpt_attn_patterns_layer_0_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Aside: <code>utils.get_act_name</code></summary>\n",
    "\n",
    "The reason these are the same is that, under the hood, the first example actually indexes by `utils.get_act_name(\"pattern\", 0)`, which evaluates to `\"blocks.0.attn.hook_pattern\"`.\n",
    "\n",
    "In general, `utils.get_act_name` is a useful function for getting the full name of an activation, given its short name and layer number.\n",
    "\n",
    "You can use the diagram from the **Transformer Architecture** section to help you find activation names.\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Attention Heads\n",
    "\n",
    "A key insight from the Mathematical Frameworks paper is that we should focus on interpreting the parts of the model that are intrinsically interpretable - the input tokens, the output logits and the attention patterns. Everything else (the residual stream, keys, queries, values, etc) are compressed intermediate states when calculating meaningful things. So a natural place to start is classifying heads by their attention patterns on various texts.\n",
    "\n",
    "When doing interpretability, it's always good to begin by visualising your data, rather than taking summary statistics. Summary statistics can be super misleading! But now that we have visualised the attention patterns, we can create some basic summary statistics and use our visualisations to validate them! (Accordingly, being good at web dev/data visualisation is a surprisingly useful skillset! Neural networks are very high-dimensional object.)\n",
    "\n",
    "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on Anthropic's PySvelte library). If you did the previous set of exercises, you'll have seen this library before.\n",
    "\n",
    "We will use the function `cv.attention.attention_patterns`, which takes the following arguments:\n",
    "\n",
    "* `attention`: Attention head activations. \n",
    "    * This should be a tensor of shape `[nhead, seq_dest, seq_src]`, i.e. the `[i, :, :]`th element is the grid of attention patterns (probabilities) for the `i`th attention head.\n",
    "    * We get this by indexing our `gpt2_cache` object.\n",
    "* `tokens`: List of tokens (e.g. `[\"A\", \"person\"]`). \n",
    "    * Sequence length must match that inferred from `attention`.\n",
    "    * This is used to label the grid.\n",
    "    * We get this by using the `gpt2_small.to_str_tokens` method.\n",
    "* `attention_head_names`: Optional list of names for the heads.\n",
    "\n",
    "There are also other circuitsvis functions, e.g. `cv.attention.attention_pattern` (for just a single head) or `cv.attention.attention_heads` (which has the same syntax and but presents the information in a different form).\n",
    "\n",
    "<details>\n",
    "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly (e.g. they continually shrink after I plot them).</summary>\n",
    "\n",
    "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
    "\n",
    "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
    "\n",
    "```python\n",
    "attn_heads = cv.attention.attention_heads(\n",
    "    tokens=gpt2_str_tokens, \n",
    "    attention=attention_pattern,\n",
    "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
    ")\n",
    "\n",
    "path = \"attn_heads.html\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(str(attn_heads))\n",
    "\n",
    "webbrowser.open(path)\n",
    "```\n",
    "\n",
    "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
    "</details>\n",
    "\n",
    "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gpt_cache))\n",
    "attention_pattern = gpt_cache[\"pattern\", 10]\n",
    "print(attention_pattern.shape)\n",
    "str_toknes = gpt2_small.to_str_tokens(text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(\n",
    "    cv.attention.attention_patterns(\n",
    "        tokens=str_toknes,\n",
    "        attention=attention_pattern\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hover over heads to see the attention patterns; click on a head to lock it. Hover over each token to see which other tokens it attends to (or which other tokens attend to it - you can see this by changing the dropdown to `Destination <- Source`).\n",
    "\n",
    "<details>\n",
    "<summary>Other circuitsvis functions - neuron activations</summary>\n",
    "\n",
    "The `circuitsvis` library also has a number of cool visualisations for **neuron activations**. Here are some more of them (you don't have to understand them all now, but you can come back to them later).\n",
    "\n",
    "The function below visualises neuron activations. The example shows just one sequence, but it can also show multiple sequences (if `tokens` is a list of lists of strings, and `activations` is a list of tensors).\n",
    "\n",
    "```python\n",
    "neuron_activations_for_all_layers = t.stack([\n",
    "    gpt2_cache[\"post\", layer] for layer in range(gpt2_small.cfg.n_layers)\n",
    "], dim=1)\n",
    "# shape = (seq_pos, layers, neurons)\n",
    "\n",
    "cv.activations.text_neuron_activations(\n",
    "    tokens=gpt2_str_tokens,\n",
    "    activations=neuron_activations_for_all_layers\n",
    ")\n",
    "```\n",
    "\n",
    "The next function shows which words each of the neurons activates most / least on (note that it requires some weird indexing to work correctly).\n",
    "\n",
    "```python\n",
    "neuron_activations_for_all_layers_rearranged = utils.to_numpy(einops.rearrange(neuron_activations_for_all_layers, \"seq layers neurons -> 1 layers seq neurons\"))\n",
    "\n",
    "cv.topk_tokens.topk_tokens(\n",
    "    # Some weird indexing required here ¯\\_(ツ)_/¯\n",
    "    tokens=[gpt2_str_tokens], \n",
    "    activations=neuron_activations_for_all_layers_rearranged,\n",
    "    max_k=7, \n",
    "    first_dimension_name=\"Layer\", \n",
    "    third_dimension_name=\"Neuron\",\n",
    "    first_dimension_labels=list(range(12))\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "# Finding induction heads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             \n",
    "Use the [diagram at this link](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/small-merm.svg) to remind yourself of the relevant hook names.\n",
    "\n",
    "\n",
    "### Exercise - visualise attention patterns\n",
    "\n",
    "```yaml\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to ~10 minutes on this exercise.\n",
    "\n",
    "It's important to be comfortable using circuitsvis, and the cache object.\n",
    "```\n",
    "\n",
    "*This exercise should be very quick - you can reuse code from the previous section. You should look at the solution if you're still stuck after 5-10 minutes.*\n",
    "\n",
    "Visualise the attention patterns for both layers of your model, on the following prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - visualize attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "We visualise attention patterns with the following code:\n",
    "\n",
    "```python\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = cache[\"pattern\", layer]\n",
    "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "```\n",
    "\n",
    "We notice that there are three basic patterns which repeat quite frequently:\n",
    "\n",
    "* `prev_token_heads`, which attend mainly to the previous token (e.g. head `0.7`)\n",
    "* `current_token_heads`, which attend mainly to the current token (e.g. head `1.6`)\n",
    "* `first_token_heads`, which attend mainly to the first token (e.g. heads `0.3` or `1.4`, although these are a bit less clear-cut than the other two)\n",
    "\n",
    "The `prev_token_heads` and `current_token_heads` are perhaps unsurprising, because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction). \n",
    "\n",
    "The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1).\n",
    "</details>\n",
    "\n",
    "\n",
    "Now that we've observed our three basic attention patterns, it's time to make detectors for those patterns!\n",
    "\n",
    "\n",
    "### Exercise - write your own detectors\n",
    "\n",
    "```yaml\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You shouldn't spend more than 15-20 minutes on these exercises.\n",
    "\n",
    "These exercises shouldn't be too challenging, if you understand attention patterns. Use the hint if stuck on things like how to correctly index your tensors, or how to access the activation patterns from the cache.\n",
    "```\n",
    "\n",
    "You should fill in the functions below, which act as detectors for particular types of heads. Validate your detectors by comparing these results to the visual attention patterns above - summary statistics on their own can be dodgy, but are much more reliable if you can validate it by directly playing with the data.\n",
    "\n",
    "Tasks like this are useful, because we need to be able to take our observations / intuitions about what a model is doing, and translate these into quantitative measures. As the exercises proceed, we'll be creating some much more interesting tools and detectors!\n",
    "\n",
    "Note - there's no objectively correct answer for which heads are doing which tasks, and which detectors can spot them. You should just try and come up with something plausible-seeming, which identifies the kind of behaviour you're looking for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_over_condition(tensor, condition):\n",
    "    I, J, K = tensor.shape\n",
    "    return [\n",
    "        sum(tensor[i, j, k] for j in range(J) for k in range(K) if condition(j, k))\n",
    "        / sum(condition(j, k) for j in range(J) for k in range(K))\n",
    "        for i in range(I)\n",
    "    ]\n",
    "\n",
    "\n",
    "def over_threshhold_attn(cache, condition, threshhold=0.5, sorce=\"pattern\"):\n",
    "    return_values = []\n",
    "\n",
    "    for layer, pattern in enumerate( cache.stack_activation(\"pattern\")):\n",
    "        scores = average_over_condition(pattern, condition)\n",
    "        indices = [i for i, s in enumerate(scores) if s > threshhold]\n",
    "        for i in indices:\n",
    "            return_values.append(f\"L{layer+1}H{i}\")\n",
    "    return return_values\n",
    "\n",
    "\n",
    "def current_attn_detector(cache: ActivationCache, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache,threshhold =.3 ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def find_repeating_rows(tensor):\n",
    "    \"\"\"\n",
    "    Finds repeating rows (vectors) in a 2D torch tensor.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): A 2D torch tensor.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the indices of repeating rows,\n",
    "          and values are the indices where those rows last occurred.\n",
    "    \"\"\"\n",
    "    last_occurrence = {}\n",
    "    repeats = {}\n",
    "\n",
    "    for pos, token in enumerate(tensor[0]):\n",
    "        id  = token.item()\n",
    "\n",
    "        if id in last_occurrence:\n",
    "            repeats[pos] = last_occurrence[id]\n",
    "        last_occurrence[id] = pos\n",
    "    \n",
    "    return repeats\n",
    "\n",
    "def induction_attn_detector(cache: ActivationCache, tokens, off_by_one = True, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
    "\n",
    "    Remember - the tokens used to generate rep_cache are (bos_token, *rand_tokens, *rand_tokens)\n",
    "    \"\"\"\n",
    "    repeat_dict = find_repeating_rows(t.tensor(tokens))\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "test_current_attn_detector(current_attn_detector, toy_transformer)\n",
    "test_first_attn_detector(first_attn_detector, toy_transformer)\n",
    "test_prev_attn_detector(prev_attn_detector, toy_transformer)\n",
    "test_induction_attn_detector(induction_attn_detector, toy_transformer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "def average_over_condition(tensor, condition):\n",
    "    I, J, K = tensor.shape\n",
    "    return [\n",
    "        sum(tensor[i, j, k] for j in range(J) for k in range(K) if condition(j, k))\n",
    "        / sum(condition(j, k) for j in range(J) for k in range(K))\n",
    "        for i in range(I)\n",
    "    ]\n",
    "\n",
    "\n",
    "def over_threshhold_attn(cache, condition, threshhold=0.5, sorce=\"pattern\"):\n",
    "    return_values = []\n",
    "\n",
    "    for layer, pattern in enumerate( cache.stack_activation(\"pattern\")):\n",
    "        scores = average_over_condition(pattern, condition)\n",
    "        indices = [i for i, s in enumerate(scores) if s > threshhold]\n",
    "        for i in indices:\n",
    "            return_values.append(f\"L{layer+1}H{i}\")\n",
    "    return return_values\n",
    "\n",
    "\n",
    "def current_attn_detector(cache: ActivationCache, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return i == j\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache,threshhold =.3 ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return i - j == 1\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(i, j):\n",
    "        return j == 0\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "\n",
    "def find_repeating_rows(tensor):\n",
    "    \"\"\"\n",
    "    Finds repeating rows (vectors) in a 2D torch tensor.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): A 2D torch tensor.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are the indices of repeating rows,\n",
    "          and values are the indices where those rows last occurred.\n",
    "    \"\"\"\n",
    "    last_occurrence = {}\n",
    "    repeats = {}\n",
    "\n",
    "    for pos, token in enumerate(tensor[0]):\n",
    "        id  = token.item()\n",
    "\n",
    "        if id in last_occurrence:\n",
    "            repeats[pos] = last_occurrence[id]\n",
    "        last_occurrence[id] = pos\n",
    "    \n",
    "    return repeats\n",
    "\n",
    "def induction_attn_detector(cache: ActivationCache, tokens, off_by_one = True, threshhold = .3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
    "\n",
    "    Remember - the tokens used to generate rep_cache are (bos_token, *rand_tokens, *rand_tokens)\n",
    "    \"\"\"\n",
    "    repeat_dict = find_repeating_rows(t.tensor(tokens))\n",
    "\n",
    "    def cond(i, j):\n",
    "        if i not in repeat_dict.keys():\n",
    "            return False\n",
    "        to_add = 1 if off_by_one else 0\n",
    "        return repeat_dict[i] + to_add == j\n",
    "\n",
    "    return over_threshhold_attn(cache, cond, threshhold=threshhold)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(gpt_cache)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(gpt_cache)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(gpt_cache)))\n",
    "print(\"Heads attending to previous mention = \", \", \".join(induction_attn_detector(gpt_cache, gpt_tokens, off_by_one=False)))\n",
    "print(\"Heads attending to one after previous mention = \", \", \".join(induction_attn_detector(gpt_cache, gpt_tokens, off_by_one=True)))\n",
    "\n",
    "\n",
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(toymodel_cache)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(toymodel_cache)))\n",
    "print(\"Heads attending to previous mention = \", \", \".join(induction_attn_detector(toymodel_cache, toymodel_tokens, off_by_one=False)))\n",
    "print(\"Heads attending to one after previous mention = \", \", \".join(induction_attn_detector(toymodel_cache, toymodel_tokens, off_by_one=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Try and compute the average attention probability along the relevant tokens. For instance, you can get the tokens just below the diagonal by using `t.diagonal` with appropriate `offset` parameter:\n",
    "\n",
    "```python\n",
    ">>> arr = t.arange(9).reshape(3, 3)\n",
    ">>> arr\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8]])\n",
    "\n",
    ">>> arr.diagonal()\n",
    "tensor([0, 4, 8])\n",
    "\n",
    ">>> arr.diagonal(-1)\n",
    "tensor([3, 7])\n",
    "```\n",
    "\n",
    "Remember that you should be using `cache[\"pattern\", layer]` to get all the attention probabilities for a given layer, and then indexing on the 0th dimension to get the correct head.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution </code></summary>\n",
    "\n",
    "```python\n",
    "def current_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of diagonal elements\n",
    "            score = attention_pattern.diagonal().mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of sub-diagonal elements\n",
    "            score = attention_pattern.diagonal(-1).mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of 0th elements\n",
    "            score = attention_pattern[:, 0].mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "```\n",
    "\n",
    "Note - choosing `0.4` as a threshold is a bit arbitrary, but it seems to work well enough. In this particular case, a threshold of `0.5` results in no head being classified as a current-token head.\n",
    "</details>\n",
    "\n",
    "\n",
    "Compare the printouts to your attention visualisations above. Do they seem to make sense?\n",
    "Play around with the threshhold and see what happens\n",
    "\n",
    "# Logit Attribution\n",
    "A consequence of the residual stream is that the output logits are the sum of the contributions of each layer, and thus the sum of the results of each head. This means we can decompose the output logits into a term coming from each head and directly do attribution like this!\n",
    "\n",
    "\n",
    "Your mission here is to write a function to look at how much each component contributes to the correct logit. Your components are:\n",
    "\n",
    "* The direct path (i.e. the residual connections from the embedding to unembedding), for each attention head\n",
    "\n",
    "To emphasise, these are not paths from the start to the end of the model, these are paths from the output of some component directly to the logits - we make no assumptions about how each path was calculated!\n",
    "\n",
    "A few important notes for this exercise:\n",
    "\n",
    "* Here we are just looking at the DIRECT effect on the logits, i.e. the thing that this component writes / embeds into the residual stream - if heads compose with other heads and affect logits like that, or inhibit logits for other tokens to boost the correct one we will not pick up on this!\n",
    "* By looking at just the logits corresponding to the correct token, our data is much lower dimensional because we can ignore all other tokens other than the correct next one (Dealing with a 50K vocab size is a pain!). But this comes at the cost of missing out on more subtle effects, like a head suppressing other plausible logits, to increase the log prob of the correct one.\n",
    "    * There are other situations where our job might be easier. For instance, in the IOI task (which we'll discuss shortly) we're just comparing the logits of the indirect object to the logits of the direct object, meaning we can use the **difference between these logits**, and ignore all the other logits.\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Question - why don't we do this to the log probs instead?</summary>\n",
    "\n",
    "Because log probs aren't linear, they go through `log_softmax`, a non-linear function.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise - build logit attribution tool\n",
    "\n",
    "\n",
    "\n",
    "You should implement the `logit_attribution` function below. This should return the contribution of each component in the \"correct direction\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_attribution_pattern(attribution_scores: Float[Tensor, \"layers heads\"]):\n",
    "    num_layers, num_heads = attribution_scores.shape\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attribution_scores,\n",
    "        x=[f\"Head {i+1}\" for i in range(num_heads)],\n",
    "        y=[f\"Layer {i+1}\" for i in range(num_layers)],\n",
    "        colorscale='Viridis',\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"Attribution Scores\",\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        xaxis_title=\"Heads\",\n",
    "        yaxis_title=\"Layers\",\n",
    "        xaxis={'tickfont': {'size': 12}, 'tickangle': -45},\n",
    "        yaxis={'tickfont': {'size': 12}},\n",
    "        coloraxis_colorbar={\n",
    "            'title': '',\n",
    "            'tickfont': {'size': 12}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_attribution(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    cache: ActivationCache,\n",
    "    token_position: int\n",
    ") -> Float[Tensor, \"layers heads\"]:\n",
    "    \"\"\"\n",
    "    Computes the logit attribution for a specific token position in the input sequence.\n",
    "\n",
    "    Args:\n",
    "        tokens (Int[Tensor, \"batch seq\"]): The input token IDs tensor with shape (batch_size, sequence_length).\n",
    "        model (HookedTransformer): The HookedTransformer model instance.\n",
    "        cache (ActivationCache): The activation cache containing the intermediate results.\n",
    "        token_position (int): The position of the token in the input sequence for which to compute the attribution.\n",
    "\n",
    "    Returns:\n",
    "        Float[Tensor, \"layers heads\"]: The logit attribution tensor with shape (num_layers, num_heads).\n",
    "\n",
    "    Description:\n",
    "        This function computes the logit attribution for a specific token position in the input sequence.\n",
    "        It unembeds the output of each attention head in each layer, and sees what upweight it gives on the correct next token.\n",
    "\n",
    "    Note:\n",
    "        - The input `tokens` tensor is assumed to have a batch size of 1.\n",
    "        - The `token_position` is zero-indexed, meaning the first token in the sequence has a position of 0.\n",
    "        - The returned attention pattern has shape (num_layers, num_heads), representing the attribution scores\n",
    "          for each layer and attention head.\n",
    "    \"\"\"\n",
    "    # Retrieve the attention results from the activation cache for each transformer block\n",
    "\n",
    "    # Hide: all\n",
    "    results = ________\n",
    "    # Hide: none\n",
    "\n",
    "    # Stack the attention results along the layer dimension\n",
    "\n",
    "    # Hide: all\n",
    "    results = t.stack________\n",
    "    # Hide: none\n",
    "    \n",
    "    # Select the attention results corresponding to the specified token position\n",
    "\n",
    "  \n",
    "    # Pass the selected attention results through the model's unembed function to obtain the logits\n",
    "\n",
    "    # Hide: all\n",
    "    logits = ________\n",
    "    # Hide: none\n",
    "    \n",
    "    # Get the ID of the next token in the sequence\n",
    "\n",
    "    # Hide: all\n",
    "    next_token_id = ________\n",
    "    # Hide: none\n",
    "    \n",
    "    # Extract the logits corresponding to the next token ID\n",
    "\n",
    "    # Hide: all\n",
    "    attributions = ________\n",
    "    # Hide: none\n",
    "    \n",
    "    return attributions\n",
    "test_logit_attribution(logit_attribution, toy_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "def logit_attribution(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    cache: ActivationCache,\n",
    "    token_position: int\n",
    ") -> Float[Tensor, \"layers heads\"]:\n",
    "    \"\"\"\n",
    "    Computes the logit attribution for a specific token position in the input sequence.\n",
    "\n",
    "    Args:\n",
    "        tokens (Int[Tensor, \"batch seq\"]): The input token IDs tensor with shape (batch_size, sequence_length).\n",
    "        model (HookedTransformer): The HookedTransformer model instance.\n",
    "        cache (ActivationCache): The activation cache containing the intermediate results.\n",
    "        token_position (int): The position of the token in the input sequence for which to compute the attribution.\n",
    "\n",
    "    Returns:\n",
    "        Float[Tensor, \"layers heads\"]: The logit attribution tensor with shape (num_layers, num_heads).\n",
    "\n",
    "    Description:\n",
    "        This function computes the logit attribution for a specific token position in the input sequence.\n",
    "        It unembeds the output of each attention head in each layer, and sees what upweight it gives on the correct next token.\n",
    "\n",
    "    Note:\n",
    "        - The input `tokens` tensor is assumed to have a batch size of 1.\n",
    "        - The `token_position` is zero-indexed, meaning the first token in the sequence has a position of 0.\n",
    "        - The returned attention pattern has shape (num_layers, num_heads), representing the attribution scores\n",
    "          for each layer and attention head.\n",
    "    \"\"\"\n",
    "    # Retrieve the attention results from the activation cache for each transformer block\n",
    "\n",
    "    # Hide: all\n",
    "    results = [cache[f\"blocks.{i}.attn.hook_result\"] for i in range(len(model.blocks))]\n",
    "    # Hide: none\n",
    "\n",
    "    # Stack the attention results along the layer dimension\n",
    "\n",
    "    # Hide: all\n",
    "    results = t.stack(results, dim=1)\n",
    "    # Hide: none\n",
    "    \n",
    "    # Select the attention results corresponding to the specified token position\n",
    "\n",
    "    # Hide: all\n",
    "    results = results[token_position, :, :, :]\n",
    "    # Hide: none\n",
    "    \n",
    "    # Pass the selected attention results through the model's unembed function to obtain the logits\n",
    "\n",
    "    # Hide: all\n",
    "    logits = model.unembed(results)\n",
    "    # Hide: none\n",
    "    \n",
    "    # Get the ID of the next token in the sequence\n",
    "\n",
    "    # Hide: all\n",
    "    next_token_id = tokens[0, token_position + 1]\n",
    "    # Hide: none\n",
    "    \n",
    "    # Extract the logits corresponding to the next token ID\n",
    "\n",
    "    # Hide: all\n",
    "    attributions = logits[:, :, next_token_id]\n",
    "    # Hide: none\n",
    "    \n",
    "    return attributions\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that you have to tool, to see wich heads have wich effects on the logits, run some experiments to see, wich heads are usefull for induction.\n",
    "Are they the same, that have the pattern you identified earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seq = t.randint(1000, 10000, (1, 10))\n",
    "repeat_seq = t.cat([rand_seq, rand_seq], dim=1)\n",
    "\n",
    "# Hide: hard\n",
    "gpt_logits, gpt_cache = gpt2_small.run_with_cache(repeat_seq, remove_batch_dim=True)\n",
    "toymodel_logits, toymodel_cache = toy_transformer.run_with_cache(repeat_seq, remove_batch_dim=True)\n",
    "\n",
    "token_postion = 14\n",
    "attribution_scores = logit_attribution(repeat_seq, toy_transformer, toymodel_cache, token_postion)\n",
    "plot_attribution_pattern(attribution_scores)\n",
    "# Hide: none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
